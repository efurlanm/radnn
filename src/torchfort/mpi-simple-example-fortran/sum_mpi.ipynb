{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d367869-67d0-444a-8693-de5727387361",
   "metadata": {},
   "source": [
    "# MPI example - Torchfort\n",
    "\n",
    "2025-06-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d7caa7d-c370-41b1-af64-8bf68852990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFT=\"~/containers/torchfort.sif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3223328c-610c-4174-90ef-eddc0c36d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maintainer: NVIDIA CORPORATION <cudatools@nvidia.com>\n",
      "org.label-schema.build-arch: amd64\n",
      "org.label-schema.build-date: Tuesday_27_May_2025_16:26:52_-03\n",
      "org.label-schema.schema-version: 1.0\n",
      "org.label-schema.usage.singularity.deffile.bootstrap: docker-daemon\n",
      "org.label-schema.usage.singularity.deffile.from: torchfort:latest\n",
      "org.label-schema.usage.singularity.version: 4.3.1-jammy\n",
      "org.opencontainers.image.ref.name: ubuntu\n",
      "org.opencontainers.image.version: 22.04\n"
     ]
    }
   ],
   "source": [
    "! singularity inspect {TFT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947a860b-6095-490e-b118-13fab3e4fb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpiexec (OpenRTE) 4.1.7a1\n",
      "\n",
      "Report bugs to http://www.open-mpi.org/community/help/\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv {TFT} mpiexec --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfec09ef-a3f6-4565-8d65-15298a88f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nvfortran 24.1-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
      "NVIDIA Compilers and Tools\n",
      "Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv {TFT} mpif90 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7689b319-8bb1-46f0-8cb2-6c8b7cbf091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm sum_mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46276872-6ff5-43d4-945b-72ab85b27ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! singularity exec --nv {TFT} mpif90 -o sum_mpi sum_mpi.f90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe5b0263-0a77-4859-b52b-afbf824a554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Process             2  (of             4 ) has local_sum =            78\n",
      " Process             0  (of             4 ) has local_sum =            55\n",
      " Process             3  (of             4 ) has local_sum =            91\n",
      " Process             1  (of             4 ) has local_sum =            66\n",
      " ----------------------------------\n",
      " Global sum calculated by root (rank 0):           290\n",
      " ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv {TFT} mpirun -np 4 ./sum_mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ba00c3-ea65-44c8-9b08-984e04a6d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp sum_mpi /scratch${HOME#/prj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13d4d44d-a688-4428-9a1c-49b2c0b9a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ~/containers/torchfort.sif /scratch${HOME#/prj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad0496e-a081-411f-a8a6-120734c62b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_mpi\n"
     ]
    }
   ],
   "source": [
    "! ls /scratch${HOME#/prj}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a364837-37bf-4522-94ad-b604a0c5d740",
   "metadata": {},
   "source": [
    "Queues:\n",
    "https://github.com/lncc-sered/manual-sdumont/wiki/06-%E2%80%90--Gerenciador-de-filas\n",
    "\n",
    "    Fila    sequana_cpu_dev\n",
    "    Wall-clock máximo (em horas)    0:20\n",
    "    Número mínimo de nós (núcleos+ dispositivos)    1 (48)\n",
    "    Número máximo de nós (núcleos+ dispositivos)    4 (192) \n",
    "    Número máximo de tarefas em execução por usuário    1\n",
    "    Número máximo de tarefas em fila por usuário    1\n",
    "    Custo em Unidade de Alocação (UA)   1,0\n",
    "    \n",
    "    Fila \tsequana_gpu_dev\n",
    "    Wall-clock máximo (em horas) \t0:20\n",
    "    Número mínimo de nós (núcleos+ dispositivos) \t1 (48+4)\n",
    "    Número máximo de nós (núcleos+ dispositivos) \t4 (192+16)\n",
    "    Número máximo de tarefas em execução por usuário \t1\n",
    "    Número máximo de tarefas em fila por usuário \t1\n",
    "    Custo em Unidade de Alocação (UA)   1,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3941261-86f2-4363-8397-edad4150b257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "   /petrobr/app_sequana/modulos2/openmpi/gnu/4.1.4_sequana:\n",
      "----------------------------------------------------------------------------\n",
      "whatis(\"Sets up Open MPI/SHMEM v4.1.4, compiled with GCC 9.3 and with UCX 1.13.0\n",
      ", in your enviornment\")\n",
      "prepend_path(\"PATH\",\"/scratch/app/openmpi/4.1.4_gnu/bin\")\n",
      "prepend_path(\"MANPATH\",\"/scratch/app/openmpi/4.1.4_gnu/share/man\")\n",
      "prepend_path(\"LD_LIBRARY_PATH\",\"/scratch/app/openmpi/4.1.4_gnu/lib:/usr/lib64/ps\n",
      "m2-compat\")\n",
      "prepend_path(\"PKG_CONFIG_PATH\",\"/scratch/app/openmpi/4.1.4_gnu/lib/pkgconfig\")\n",
      "setenv(\"MPI_ROOT\",\"/scratch/app/openmpi/4.1.4_gnu\")\n",
      "setenv(\"OMPI_PATH\",\"/scratch/app/openmpi/4.1.4_gnu/bin\")\n",
      "setenv(\"OMPI_LD_LIBRARY_PATH\",\"/scratch/app/openmpi/4.1.4_gnu/lib\")\n",
      "setenv(\"OMPI_MCA_btl_openib_allow_ib\",\"1\")\n",
      "setenv(\"OMPI_MCA_btl_openib_warn_default_gid_prefix\",\"0\")\n",
      "setenv(\"OMPI_MCA_btl_vader_single_copy_mechanism\",\"2\")\n",
      "prepend_path{\"LDFLAGS\",\"-L/scratch/app/openmpi/4.1.4_gnu/lib\",delim=\" \"}\n",
      "prepend_path{\"CPPFLAGS\",\"-I/scratch/app/openmpi/4.1.4_gnu/include\",delim=\" \"}\n",
      "unload(\"ucx/1.13_sequana\",\"gcc/9.3_sequana\")\n",
      "help([[This module adds Open MPI/SHMEM v4.1.4, compiled with GCC 9.3 and with UC\n",
      "X 1.13.0, to various paths\n",
      "]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! module show openmpi/gnu/4.1.4_sequana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a84d74ac-09d0-4d70-ace7-f5d4453419ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sum_mpi.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile sum_mpi.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --partition=sequana_cpu_dev   # SLURM_JOB_PARTITION\n",
    "#SBATCH --job-name=hv-tf1-mnist       # SLURM_JOB_NAME\n",
    "#SBATCH --nodes=4                     # SLURM_JOB_NUM_NODES\n",
    "#SBATCH --ntasks-per-node=1           # SLURM_NTASKS_PER_NODE\n",
    "#SBATCH --cpus-per-task=1             # SLURM_CPUS_PER_TASK\n",
    "#SBATCH --distribution=block:cyclic   # SLURM_DISTRIBUTION\n",
    "#SBATCH --time=00:05:00               # Limit execution time\n",
    "# SLURM output environment variables:\n",
    "# https://slurm.schedmd.com/sbatch.html#SECTION_OUTPUT-ENVIRONMENT-VARIABLES\n",
    "echo '======================================='\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Nº of nodes in the job:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Nº of tasks per node:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Nº of tasks:' $SLURM_NTASKS\n",
    "echo '- Nº of cpus per task:' $SLURM_CPUS_PER_TASK\n",
    "echo '- Partition:' $SLURM_JOB_PARTITION\n",
    "echo '- Dir from which sbatch was invoked:' ${SLURM_SUBMIT_DIR##*/}\n",
    "echo -n '- Nodes allocated to the job: '\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "echo '----------------------------------------'\n",
    "#cd $SLURM_SUBMIT_DIR\n",
    "cd /scratch/ampemi/eduardo.miranda2/\n",
    "# Run\n",
    "export SLURM_MPI_TYPE=pmi2\n",
    "#export OMPI_HOME=$(dirname $(dirname $(which mpirun)))\n",
    "module load openmpi/gnu/4.1.4+cuda-11.2_sequana\n",
    "echo -n '<1. starting python script > ' && date\n",
    "echo '-- output -----------------------------'\n",
    "#srun --cpu_bind=cores singularity exec torchfort.sif mpirun -np 4 ./sum_mpi\n",
    "#srun --mpi=pmi2 -n 4 hostname\n",
    "#srun singularity exec \\\n",
    "#    --bind /usr/lib64/mpi:/usr/lib64/mpi \\\n",
    "#    torchfort.sif ./sum_mpi\n",
    "#srun --mpi=none singularity exec torchfort.sif mpirun --allow-run-as-root ./sum_mpi\n",
    "srun singularity exec --nv --bind /usr/lib64:/host_libs \\\n",
    "    torchfort.sif bash -c \"export LD_LIBRARY_PATH=/host_libs:\\$LD_LIBRARY_PATH && ./sum_mpi\"\n",
    "echo '-- end --------------------------------'\n",
    "echo -n '<2. quit>                    ' && date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8eab454c-4180-4ed1-8f99-9cf3d42ff302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342271\n",
      "=======================================\n",
      "- Job ID: 11342271\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6078 sdumont6165 sdumont6166 sdumont6167\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 23:41:40 -03 2025\n",
      "-- output -----------------------------\n",
      "WARNING: Could not find any nv files on this host!\n",
      "WARNING: Could not find any nv files on this host!\n",
      "WARNING: Could not find any nv files on this host!\n",
      "WARNING: Could not find any nv files on this host!\n",
      "./sum_mpi: /host_libs/libc.so.6: version `GLIBC_2.34' not found (required by ./sum_mpi)\n",
      "./sum_mpi: /host_libs/libc.so.6: version `GLIBC_2.34' not found (required by ./sum_mpi)\n",
      "./sum_mpi: /host_libs/libc.so.6: version `GLIBC_2.34' not found (required by ./sum_mpi)\n",
      "srun: error: sdumont6167: task 3: Exited with exit code 1\n",
      "srun: error: sdumont6166: task 2: Exited with exit code 1\n",
      "srun: error: sdumont6165: task 1: Exited with exit code 1\n",
      "./sum_mpi: /host_libs/libc.so.6: version `GLIBC_2.34' not found (required by ./sum_mpi)\n",
      "srun: error: sdumont6078: task 0: Exited with exit code 1\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 23:41:40 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43577d73-3a6b-4ae4-b8f6-3de6446d1118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778fedc-a22b-4cb4-914c-57abcfccb8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ff1941-e6c8-46ee-b51c-45d52434be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342253\n",
      "=======================================\n",
      "- Job ID: 11342253\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6075 sdumont6165 sdumont6166 sdumont6167\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 23:17:44 -03 2025\n",
      "-- output -----------------------------\n",
      "srun: defined options\n",
      "srun: -------------------- --------------------\n",
      "srun: (null)              : sdumont[6075,6165-6167]\n",
      "srun: cpus-per-task       : 1\n",
      "srun: distribution        : block:cyclic\n",
      "srun: jobid               : 11342253\n",
      "srun: job-name            : hv-tf1-mnist\n",
      "srun: mem-per-cpu         : 8000\n",
      "srun: mpi                 : pmi2\n",
      "srun: nodes               : 4\n",
      "srun: ntasks              : 4\n",
      "srun: ntasks-per-node     : 1\n",
      "srun: tres-per-task       : cpu:1\n",
      "srun: verbose             : 1\n",
      "srun: -------------------- --------------------\n",
      "srun: end of defined options\n",
      "srun: jobid 11342253: nodes(4):`sdumont[6075,6165-6167]', cpu counts: 1(x4)\n",
      "srun: Implicitly setting --exact, because -c/--cpus-per-task given.\n",
      "srun: CpuBindType=(null type)\n",
      "srun: launching StepId=11342253.0 on host sdumont6075, 1 tasks: 0\n",
      "srun: launching StepId=11342253.0 on host sdumont6165, 1 tasks: 1\n",
      "srun: launching StepId=11342253.0 on host sdumont6166, 1 tasks: 2\n",
      "srun: launching StepId=11342253.0 on host sdumont6167, 1 tasks: 3\n",
      "srun: topology/default: init: topology Default plugin loaded\n",
      "srun: Node sdumont6167, 1 tasks started\n",
      "srun: Node sdumont6166, 1 tasks started\n",
      "srun: Node sdumont6165, 1 tasks started\n",
      "srun: Node sdumont6075, 1 tasks started\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "srun: Received task exit notification for 1 task of StepId=11342253.0 (status=0x0100).\n",
      "srun: error: sdumont6166: task 2: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6166:798082] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "srun: Received task exit notification for 1 task of StepId=11342253.0 (status=0x0100).\n",
      "srun: error: sdumont6167: task 3: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6167:1591602] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6165:251674] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "srun: Received task exit notification for 1 task of StepId=11342253.0 (status=0x0100).\n",
      "srun: error: sdumont6165: task 1: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6075:936226] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "srun: Received task exit notification for 1 task of StepId=11342253.0 (status=0x0100).\n",
      "srun: error: sdumont6075: task 0: Exited with exit code 1\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 23:17:45 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72307883-0e34-4250-973d-0cb6ba76b9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80054c16-bc0d-480a-a74e-bca5c06e278b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87a0d5-4853-4b9d-8561-0ac1b214f332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc6fc7a8-5978-4dad-a531-02bc841ad22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342250\n",
      "=======================================\n",
      "- Job ID: 11342250\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6075 sdumont6078 sdumont6079 sdumont6082\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 23:16:04 -03 2025\n",
      "-- output -----------------------------\n",
      "srun: defined options\n",
      "srun: -------------------- --------------------\n",
      "srun: (null)              : sdumont[6075,6078-6079,6082]\n",
      "srun: cpus-per-task       : 1\n",
      "srun: distribution        : block:cyclic\n",
      "srun: jobid               : 11342250\n",
      "srun: job-name            : hv-tf1-mnist\n",
      "srun: mem-per-cpu         : 8000\n",
      "srun: mpi                 : pmi2\n",
      "srun: nodes               : 4\n",
      "srun: ntasks              : 4\n",
      "srun: ntasks-per-node     : 1\n",
      "srun: tres-per-task       : cpu:1\n",
      "srun: verbose             : 1\n",
      "srun: -------------------- --------------------\n",
      "srun: end of defined options\n",
      "srun: jobid 11342250: nodes(4):`sdumont[6075,6078-6079,6082]', cpu counts: 1(x4)\n",
      "srun: Implicitly setting --exact, because -c/--cpus-per-task given.\n",
      "srun: CpuBindType=(null type)\n",
      "srun: launching StepId=11342250.0 on host sdumont6075, 1 tasks: 0\n",
      "srun: launching StepId=11342250.0 on host sdumont6078, 1 tasks: 1\n",
      "srun: launching StepId=11342250.0 on host sdumont6079, 1 tasks: 2\n",
      "srun: launching StepId=11342250.0 on host sdumont6082, 1 tasks: 3\n",
      "srun: topology/default: init: topology Default plugin loaded\n",
      "srun: Node sdumont6082, 1 tasks started\n",
      "srun: Node sdumont6079, 1 tasks started\n",
      "srun: Node sdumont6075, 1 tasks started\n",
      "srun: Node sdumont6078, 1 tasks started\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6079:545767] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6082:65962] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "srun: Received task exit notification for 1 task of StepId=11342250.0 (status=0x0100).\n",
      "srun: error: sdumont6082: task 3: Exited with exit code 1\n",
      "srun: Received task exit notification for 1 task of StepId=11342250.0 (status=0x0100).\n",
      "srun: error: sdumont6079: task 2: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "srun: Received task exit notification for 1 task of StepId=11342250.0 (status=0x0100).\n",
      "srun: error: sdumont6078: task 1: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6078:654678] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6075:936089] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "srun: Received task exit notification for 1 task of StepId=11342250.0 (status=0x0100).\n",
      "srun: error: sdumont6075: task 0: Exited with exit code 1\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 23:16:05 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c092c6-e596-4beb-9ce7-784cfb0b5087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4111a87-4f5b-419e-b19f-f5d51cf39f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0881ef7-5005-4292-9b2f-68d8eb397422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674264a7-e9ab-408d-882c-e13292e3efcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64279026-0ebb-4b8d-8133-3e66161144a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8004a1f-7aad-4d9a-a179-14a9a7a6b189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09c0d372-20c2-4432-af20-13f81e0ce6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342241\n",
      "=======================================\n",
      "- Job ID: 11342241\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6075 sdumont6078 sdumont6079 sdumont6082\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 23:09:33 -03 2025\n",
      "-- output -----------------------------\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[2548,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "srun: error: sdumont6079: task 2: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[17919,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[49328,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "srun: error: sdumont6075: task 0: Exited with exit code 1\n",
      "srun: error: sdumont6078: task 1: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[849,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "srun: error: sdumont6082: task 3: Exited with exit code 1\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 23:09:35 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c7cf1-6ee3-4b84-b003-fe558133c9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e008c9-dc74-44df-ab35-4b56c9d2c472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86a686-a8fe-4404-aef8-258b72ffbf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebdc4d-4a0c-47c7-905b-7ae446c45da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a03fa5d-7649-4c05-8a87-18a8b0d2575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342239\n",
      "=======================================\n",
      "- Job ID: 11342239\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6075 sdumont6165 sdumont6166 sdumont6167\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 23:08:02 -03 2025\n",
      "-- output -----------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6167:1591253] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6166:797724] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6165:251023] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "srun: error: sdumont6166: task 2: Exited with exit code 1\n",
      "srun: error: sdumont6167: task 3: Exited with exit code 1\n",
      "srun: error: sdumont6165: task 1: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    opal_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-opal-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    orte_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-orte-runtime: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Sorry!  You were supposed to get help about:\n",
      "    mpi_init:startup:internal-failure\n",
      "But I couldn't open the help file:\n",
      "    /proj/nv/libraries/Linux_x86_64/24.1/hpcx-12/243364-rel-1/comm_libs/12.2/hpcx/hpcx-2.17.1/ompi/share/openmpi/help-mpi-runtime.txt: No such file or directory.  Sorry!\n",
      "--------------------------------------------------------------------------\n",
      "*** An error occurred in MPI_Init\n",
      "*** on a NULL communicator\n",
      "*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n",
      "***    and potentially your MPI job)\n",
      "[sdumont6075:935636] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n",
      "srun: error: sdumont6075: task 0: Exited with exit code 1\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 23:08:03 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727c3c4-26cd-42e3-b637-4f1dac4c503f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cba44-dfae-43fb-9862-6fc7acd1fd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a938fc0-2d2f-45db-b254-831e23663917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342233\n",
      "=======================================\n",
      "- Job ID: 11342233\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6078 sdumont6079 sdumont6166 sdumont6167\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09390ae0-df83-4ae7-b241-acfe970858aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7b4ad-0a4f-4235-8f7f-8ed90f7fc8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02cbff-1717-41e3-92b2-c54f3468d662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40504aa0-37ca-490d-bb44-a72a97b01ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342230\n",
      "=======================================\n",
      "- Job ID: 11342230\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6075 sdumont6078 sdumont6079 sdumont6082\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 22:59:49 -03 2025\n",
      "-- output -----------------------------\n",
      "sdumont6082\n",
      "sdumont6079\n",
      "sdumont6075\n",
      "sdumont6078\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 22:59:49 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb131e61-f7da-4b3d-8f6a-699c0b0eabb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4002340-a8c8-49cb-9bf2-4efac3c40550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ad51e-36e1-4640-8f59-ba386c1cedce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b49452-715a-4e49-807c-19d871d9833c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "375f325f-218f-4596-9cdc-3948ae91b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11342214\n",
      "=======================================\n",
      "- Job ID: 11342214\n",
      "- Nº of nodes in the job: 4\n",
      "- Nº of tasks per node: 1\n",
      "- Nº of tasks: 4\n",
      "- Nº of cpus per task: 1\n",
      "- Partition: sequana_cpu_dev\n",
      "- Dir from which sbatch was invoked: mpi-simple-example-fortran\n",
      "- Nodes allocated to the job: sdumont6075 sdumont6165 sdumont6166 sdumont6167\n",
      "----------------------------------------\n",
      "<1. starting python script > qui jun  5 22:31:54 -03 2025\n",
      "-- output -----------------------------\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[15254,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[57749,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[37135,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "srun: error: sdumont6166: task 2: Exited with exit code 1\n",
      "srun: error: sdumont6165: task 1: Exited with exit code 1\n",
      "srun: error: sdumont6167: task 3: Exited with exit code 1\n",
      "--------------------------------------------------------------------------\n",
      "The SLURM process starter for OpenMPI was unable to locate a\n",
      "usable \"srun\" command in its path. Please check your path\n",
      "and try again.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "An internal error has occurred in ORTE:\n",
      "\n",
      "[[18261,0],0] FORCE-TERMINATE AT (null):1 - error ../../../../../orte/mca/plm/slurm/plm_slurm_module.c(475)\n",
      "\n",
      "This is something that should be reported to the developers.\n",
      "--------------------------------------------------------------------------\n",
      "srun: error: sdumont6075: task 0: Exited with exit code 1\n",
      "-- end --------------------------------\n",
      "<2. quit>                    qui jun  5 22:31:55 -03 2025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = ! sbatch sum_mpi.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    b = ! squeue --user=$USER --name=sum_mpi\n",
    "    if len(b) < 2 : break\n",
    "b = ! echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221da855-877a-41ec-acb3-798c23aaabdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
