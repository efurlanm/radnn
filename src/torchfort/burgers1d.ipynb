{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e54b15-284f-4fd1-b3e0-59342e5d491f",
   "metadata": {},
   "source": [
    "# 1D BURGERS TORCHFORT\n",
    "\n",
    "2025-07-08\n",
    "\n",
    "First attempt at converting burgers1d-pytorch.ipynb from PyTorch to TorchFort.\n",
    "\n",
    "So far the results have been (in parentheses are results from previous attempts, for reference only):\n",
    "\n",
    "* Maximum Absolute Difference: 1.92249e-01 (Reduced from 1.54e+00)\n",
    "* Mean Absolute Difference: 3.90952e-03 (Reduced from 2.67e-02)\n",
    "\n",
    "A mean absolute difference of 3.9e-03 is a very good result, indicating that the Fortran implementation (training and inference) is now producing results that are very similar to the original Python program. The remaining small differences are likely due to the inherent numerical variations between different compilers, floating-point implementations, and possibly subtle differences in how PyTorch's Python frontend and LibTorch's C++ backend handle certain operations, even with fixed seeds. This successfully addresses the goal of improving the comparison and ensuring the new implementation generates correct and comparable results.\n",
    "\n",
    "The conversion project is being done in stages, and the current stage is being called \"Project 04\". In the final comparison for Project 04, we compared the inference results of two different implementations of the 1D Burgers' equation PINN solver:\n",
    "\n",
    "1. Original Python Implementation (`burgers1d.py`): This script trains the PINN model entirely in Python (using PyTorch) and then performs inference on a defined spatial-temporal grid. Its predicted u(x,t) values are saved to a binary file (burgers1d_python_original_results.bin). This serves as our reference or ground truth for the comparison.\n",
    "\n",
    "2. Fortran Implementation (`burgers04.f90`): This program trains the PINN model in Fortran (leveraging the TorchFort library) and then performs inference on the exact same spatial-temporal grid. Its predicted u(x,t) values are saved to a separate binary file (burgers04_fortran_inference_results.bin).\n",
    "\n",
    "The comparison aims to determine how closely the Fortran implementation's output matches the original Python implementation's output, especially after both have been trained under identical conditions (same random seed, network architecture, and hyperparameters).\n",
    "\n",
    "## Maximum Absolute Difference and Mean Absolute Difference\n",
    "\n",
    "These are common metrics used to quantify the difference between two sets of numerical data.\n",
    "\n",
    "Let $U_{original}$ be the array of predicted $u$ values from the original Python implementation, and $U_{fortran}$ be the array of predicted $u$ values from the Fortran implementation. Both arrays are assumed to have the same shape and correspond to the same spatial-temporal grid points.\n",
    "\n",
    "1. Absolute Difference (Element-wise):\n",
    "\n",
    "For each corresponding element (or point) in the two arrays, we calculate the absolute value of their difference. If $u_{original, i}$ is the value at point $i$ in the original Python results and $u_{fortran, i}$ is the value at point $i$ in the Fortran results, the absolute difference at that point is:\n",
    "\n",
    " $ \\text{abs\\_diff}_i = |u_{original, i} - u_{fortran, i}| $\n",
    "This creates a new array (or matrix) of absolute differences.\n",
    "\n",
    "2. Maximum Absolute Difference:\n",
    "\n",
    "This metric represents the largest discrepancy between any corresponding pair of points in the two datasets. It is simply the maximum value found in the array of absolute differences.\n",
    "    $\\text{MaxAbsDiff} = \\max(\\text{abs\\_diff}_i) $\n",
    "A smaller value indicates that no single point has a significantly large deviation.\n",
    "\n",
    "3. Mean Absolute Difference:\n",
    "   This metric provides an average measure of the difference between the two datasets. It is calculated by summing all the absolute differences and dividing by the total number of points.\n",
    "   $  \\text{MeanAbsDiff} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{abs\\_diff}_i $\n",
    "   where $N$ is the total number of points. A smaller value indicates that, on average, the two datasets are very close.\n",
    "\n",
    "In our Python comparison script (compare_results_04.py), these calculations are performed using NumPy functions:\n",
    "\n",
    "* abs_diff = np.abs(U_original_python - U_fortran_04)\n",
    "* max_abs_diff = np.max(abs_diff)\n",
    "* mean_abs_diff = np.mean(abs_diff)\n",
    "\n",
    "The results showed a Mean Absolute Difference of 3.90952e-03, which is a very small value, indicating a high degree of similarity between the two implementations despite being in different languages and environments.\n",
    "\n",
    "## Run\n",
    "\n",
    " Make sure you are in the `...burgers/torchfort_local/` directory.  \n",
    "\n",
    " Workflow Execution Commands  \n",
    "\n",
    "  1. Generate Initial TorchScript Models (Python)  \n",
    "\n",
    "This creates burgers_model.pt, burgers_loss.pt, and burgers_inference_net.pt with the specified architecture and  fixed random seed.  \n",
    "\n",
    "```\n",
    "singularity exec --nv \\\n",
    "    --bind /home/x/tfort/burgers/torchfort_local:/torchfort \\\n",
    "    ~/containers/torchfort.sif bash -c \\\n",
    "    \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "    cd /torchfort/build && \\\n",
    "    python ../examples/fortran/burgers/generate_burgers_model.py\"  \n",
    "```\n",
    "\n",
    "  2. Run Original Python Burgers1D Script (Python)  \n",
    "\n",
    "This trains the original Python model and saves its inference results to burgers1d_python_original_results.bin.  \n",
    "\n",
    "```\n",
    "singularity exec --nv \\\n",
    "    --bind /home/x/tfort/burgers/torchfort_local:/torchfort \\\n",
    "    ~/containers/torchfort.sif bash -c \\\n",
    "    \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "    python /torchfort/burgers1d.py\"  \n",
    "```\n",
    "\n",
    "  3. Compile Fortran Programs (CMake/Make)  \n",
    "\n",
    "This compiles all Fortran executables, including burgers04, using the updated CMakeLists.txt.  \n",
    "\n",
    "```\n",
    "singularity exec --nv \\\n",
    "    --bind /home/x/tfort/burgers/torchfort_local:/torchfort \\\n",
    "    ~/containers/torchfort.sif bash -c \\\n",
    "    \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "    cd /torchfort/build && \\\n",
    "    make\"  \n",
    "```\n",
    "\n",
    "  4. Run Fortran Burgers04 Program (Fortran) \n",
    "\n",
    "This program trains the PINN model in Fortran and then performs inference, saving the trained model  (burgers_model_trained_04.pt) and the Fortran inference results (burgers04_fortran_inference_results.bin).  \n",
    "\n",
    "```\n",
    "singularity exec --nv \\\n",
    "    --bind /home/x/tfort/burgers/torchfort_local:/torchfort \\\n",
    "    ~/containers/torchfort.sif bash -c \\\n",
    "    \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "    cd /torchfort/build && \\\n",
    "    ./examples/fortran/burgers/burgers04\"  \n",
    "```\n",
    "\n",
    "  5. Run Python Comparison Script (Python)  \n",
    "     This script loads the results from both the original Python run and the Fortran run, performs the comparison,  and displays the results and plots.  \n",
    "\n",
    "```\n",
    "singularity exec --nv \\\n",
    "    --bind /home/x/tfort/burgers/torchfort_local:/torchfort \\\n",
    "    ~/containers/torchfort.sif bash -c \\ \n",
    "    \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "    python /torchfort/compare_results_04.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3ef07-8281-451e-a64a-2d7382cd32d8",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80400f83-92ad-4a02-ac2a-bcb4032a2b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/x/tfort/burgers/torchfort_local\n"
     ]
    }
   ],
   "source": [
    "%cd ~/tfort/burgers/torchfort_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6e46a3d-997a-4e69-bf93-7cd25b1d356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BurgersPINN model (initial): BurgersPINN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (11): Tanh()\n",
      "    (12): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (15): Tanh()\n",
      "    (16): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Identity Loss module (initial): IdentityLoss()\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv --bind ~/tfort/burgers/torchfort_local:/torchfort \\\n",
    "     ~/containers/torchfort.sif bash -c \\\n",
    "     \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "     cd /torchfort/build && \\\n",
    "     python3 ../examples/fortran/burgers/generate_burgers_model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a83d3a40-dd5f-4036-b6ed-cad627309a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Epoch 5000/50000, Loss: 4.08371e-03\n",
      "Epoch 10000/50000, Loss: 1.79177e-03\n",
      "Epoch 15000/50000, Loss: 1.10168e-03\n",
      "Epoch 20000/50000, Loss: 9.80160e-04\n",
      "Epoch 25000/50000, Loss: 8.24852e-04\n",
      "Epoch 30000/50000, Loss: 4.00022e-04\n",
      "Epoch 35000/50000, Loss: 3.21175e-04\n",
      "Epoch 40000/50000, Loss: 2.63482e-04\n",
      "Epoch 45000/50000, Loss: 2.41118e-04\n",
      "Epoch 50000/50000, Loss: 1.74650e-04\n",
      "Training complete!\n",
      "Original Python inference results saved to burgers1d_python_original_results.bin\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv --bind ~/tfort/burgers/torchfort_local:/torchfort \\\n",
    "     ~/containers/torchfort.sif bash -c \\\n",
    "     \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "     cd /torchfort/build && \\\n",
    "     python3 /torchfort/burgers1d.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a98928-b596-462a-a36a-7d3fb38b57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 56%] Built target torchfort\n",
      "[ 60%] Built target torchfort_fort\n",
      "[ 63%] Built target environments\n",
      "[ 67%] Built target train_cart_pole\n",
      "[ 70%] Built target PyEnvironments\n",
      "[ 76%] Built target train\n",
      "[ 81%] Built target train_distributed\n",
      "[ 85%] Built target train_graph\n",
      "[ 89%] Built target burgers\n",
      "[ 92%] Built target burgers02\n",
      "[ 96%] Built target burgers03\n",
      "[100%] Built target burgers04\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv --bind ~/tfort/burgers/torchfort_local:/torchfort \\\n",
    "     ~/containers/torchfort.sif bash -c \\\n",
    "     \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "     cd /torchfort/build && \\\n",
    "     make\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aaea07f-1594-4bef-9160-1e1b44f7e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training model created successfully\n",
      " Epoch:          5000 , Loss:    6.3064985E-04\n",
      " Epoch:         10000 , Loss:    2.6819861E-04\n",
      " Epoch:         15000 , Loss:    2.0657627E-04\n",
      " Epoch:         20000 , Loss:    1.3856328E-04\n",
      " Epoch:         25000 , Loss:    1.0746127E-04\n",
      " Epoch:         30000 , Loss:    4.3721253E-04\n",
      " Epoch:         35000 , Loss:    8.4690902E-05\n",
      " Epoch:         40000 , Loss:    8.8860696E-05\n",
      " Epoch:         45000 , Loss:    8.4965446E-05\n",
      " Epoch:         50000 , Loss:    7.2721698E-05\n",
      " Training completed. Final Loss:    7.2721698E-05\n",
      " Trained model saved successfully to \n",
      " ../examples/fortran/burgers/burgers_model_trained_04.pt                                                                                                                                                                                                         \n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv --bind ~/tfort/burgers/torchfort_local:/torchfort \\\n",
    "     ~/containers/torchfort.sif bash -c \\\n",
    "     \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "     cd /torchfort/build && \\\n",
    "     ./examples/fortran/burgers/burgers04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5284bbc-172d-41cb-9bfb-c64509442295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Python results loaded from burgers1d_python_original_results.bin\n",
      "Original Python U_pred shape: (100, 256)\n",
      "Fortran-trained model loaded from /torchfort/examples/fortran/burgers/burgers_model_trained_04.pt\n",
      "Inference with Fortran-trained model performed. U_fortran_trained shape: (100, 256)\n",
      "\n",
      "Comparison Results (Original Python vs. Fortran-trained Python):\n",
      "  Maximum Absolute Difference: 1.92249e-01\n",
      "  Mean Absolute Difference: 3.90952e-03\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv --bind ~/tfort/burgers/torchfort_local:/torchfort \\\n",
    "     ~/containers/torchfort.sif bash -c \\\n",
    "     \"CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda && \\\n",
    "     cd /torchfort/build && \\\n",
    "     python3 /torchfort/compare_results_04.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab9cf1-96fd-490d-9834-43da9ad9e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382293e9-b83b-4601-bd21-e20507dc40c0",
   "metadata": {},
   "source": [
    "Excluding the matplotlib part (visualization), the core logic of the 1D\n",
    "  Burgers' equation PINN solver from @burgers1d-pytorch.ipynb has been\n",
    "  completely implemented in @examples/fortran/burgers/burgers04.f90, now\n",
    "  encompassing both training and inference within Fortran.\n",
    "\n",
    "1. Model Definition and Loss Logic (PINN and BurgersLoss):\n",
    "   * In the Python notebook, the PINN (neural network) and BurgersLoss\n",
    "      (loss function with automatic differentiation for the Burgers\n",
    "     equation, initial, and boundary conditions) classes are defined.\n",
    "   * During the conversion process, these two logics were combined\n",
    "     into the BurgersPINN class in the generate_burgers_model.py\n",
    "     script. This class is then exported as a TorchScript model\n",
    "     (burgers_model.pt).\n",
    "   * The IdentityLoss (also exported as burgers_loss.pt) serves merely\n",
    "      as a \"pass-through\" for the loss already calculated by\n",
    "     BurgersPINN.\n",
    "   * Additionally, a SimpleInferenceNet (a simplified version of the\n",
    "     neural network for direct prediction) is also exported as\n",
    "     burgers_inference_net.pt.\n",
    "   * The Fortran code loads these TorchScript models, and when\n",
    "     torchfort_train_multiarg is called, the neural network logic and\n",
    "     the loss calculation (including automatic differentiation) are\n",
    "     executed by the TorchFort backend (LibTorch).\n",
    "2. Data Generation:\n",
    "   * The logic for generating collocation points, initial conditions,\n",
    "     and boundary conditions is directly replicated in Fortran, using\n",
    "     the same formulas and parameters as in the Python notebook.\n",
    "3. Training Loop and Optimization:\n",
    "   * The training loop in Fortran iterates for a defined number of\n",
    "     epochs, calling torchfort_train_multiarg.\n",
    "   * The optimizer (Adam) and learning rate are configured in the\n",
    "     config_burgers.yaml file, which is read by TorchFort, mirroring\n",
    "     the optimizer setup in the Python notebook.\n",
    "4. Inference Execution in Fortran:\n",
    "   * After training, burgers04.f90 creates a separate TorchFort model\n",
    "     instance configured to load burgers_inference_net.pt.\n",
    "   * It then loads the trained weights from the main burgers_model.pt\n",
    "     into this inference-specific model.\n",
    "   * Inference is performed on a defined spatial-temporal grid using\n",
    "     torchfort_inference_multiarg, and the results are saved to a\n",
    "     binary file (burgers04_fortran_inference_results.bin).\n",
    "\n",
    "\n",
    "  In summary, Fortran now handles both the complex neural network\n",
    "  training and the subsequent inference process without re-implementing\n",
    "  the neural network or automatic differentiation from scratch. Instead,\n",
    "   it uses the TorchFort API to interact with the pre-compiled PyTorch\n",
    "  models (TorchScript) that contain all this logic. This allows the\n",
    "  Fortran program to execute the same training and inference processes\n",
    "  as the Python notebook, without needing to rewrite the complexities of\n",
    "   deep learning in Fortran."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c3b55-28b8-413e-96bc-c618d8e3d6bb",
   "metadata": {},
   "source": [
    "## The Python script\n",
    "\n",
    "\n",
    "The generate_burgers_model.py script is a Python program designed to\n",
    "create and export TorchScript models (.pt files) that are then used by\n",
    "the Fortran application via the TorchFort library. Its primary\n",
    "purpose is to encapsulate the Physics-Informed Neural Network (PINN)\n",
    "and its associated loss calculation, along with a dedicated inference\n",
    "network, into TorchScript format.\n",
    "\n",
    "Here's a breakdown of its key components:\n",
    "\n",
    "1. `BurgersPINN` Class:\n",
    "   * Purpose: This class represents the core of the Physics-Informed\n",
    "     Neural Network for solving the 1D Burgers' equation. Unlike a\n",
    "     traditional neural network that only predicts the solution, this\n",
    "     PINN also incorporates the partial differential equation (PDE)\n",
    "     and boundary/initial conditions directly into its loss\n",
    "     calculation.\n",
    "   * `__init__` method: Initializes a feed-forward neural network\n",
    "     (self.net) with 8 hidden layers, each with 10 neurons, using\n",
    "     torch.nn.Linear layers and Tanh activation functions. It also\n",
    "     stores the nu (viscosity) parameter of the Burgers' equation.\n",
    "   * `forward` method: This is the crucial part where the PINN's logic\n",
    "      resides. It takes several input tensors:\n",
    "       * t_collocation, x_collocation: Points where the PDE residual\n",
    "         will be minimized.\n",
    "       * t_initial, x_initial, u_initial_true: Data for the initial\n",
    "         condition.\n",
    "       * t_boundary, x_boundary, u_boundary_true: Data for the\n",
    "         boundary conditions.\n",
    "       * PDE Loss Calculation:\n",
    "           * It first uses the neural network (self.net) to predict\n",
    "             u_collocation (the solution u) at the collocation points.\n",
    "           * It then leverages torch.autograd.grad (PyTorch's\n",
    "             automatic differentiation engine) to compute the\n",
    "             necessary derivatives: u_t (time derivative of u), u_x\n",
    "             (first spatial derivative of u), and u_xx (second spatial\n",
    "              derivative of u). This is where the \"physics-informed\"\n",
    "             aspect comes in, as it calculates how well the network's\n",
    "             output satisfies the PDE.\n",
    "           * The PDE residual f is computed (u_t + u_collocation * u_x\n",
    "              - self.nu * u_xx).\n",
    "           * loss_f is the mean squared error of this residual, aiming\n",
    "              to drive it to zero.\n",
    "       * Initial Condition Loss (`loss_i`): Calculates the mean\n",
    "         squared error between the network's prediction at initial\n",
    "         condition points (u_initial_pred) and the true initial values\n",
    "          (u_initial_true).\n",
    "       * Boundary Condition Loss (`loss_b`): Calculates the mean\n",
    "         squared error between the network's prediction at boundary\n",
    "         condition points (u_boundary_pred) and the true boundary\n",
    "         values (u_boundary_true).\n",
    "       * Return Value: The method returns the sum of loss_f, loss_i,\n",
    "         and loss_b, representing the total loss that needs to be\n",
    "         minimized during training.\n",
    "2. `IdentityLoss` Class:\n",
    "   * Purpose: This is a simple, dummy loss function.\n",
    "   * `forward` method: It takes two arguments (model_output and\n",
    "     dummy_label) but simply returns the model_output.\n",
    "   * Why it's needed: In the TorchFort Fortran API, the\n",
    "     torchfort_train_multiarg function expects both a model and a\n",
    "     separate loss function. Since the BurgersPINN model already\n",
    "     calculates and returns the total loss directly from its forward\n",
    "     method, this IdentityLoss acts as a placeholder. It receives the\n",
    "     loss value (as model_output) from BurgersPINN and passes it\n",
    "     through without further modification, fulfilling the API's\n",
    "     requirement.\n",
    "3. `SimpleInferenceNet` Class:\n",
    "   * Purpose: This class is a lightweight wrapper specifically\n",
    "     designed for inference. It encapsulates only the neural network\n",
    "     part (self.net) of the BurgersPINN model.\n",
    "   * `__init__` method: Takes a torch.nn.Sequential module (which is\n",
    "     BurgersPINN.net) as input and stores it.\n",
    "   * `forward` method: This simplified method takes only t and x\n",
    "     tensors as input, concatenates them, and passes them through the\n",
    "     internal neural network (self.net) to predict the solution u.\n",
    "     This ensures a clean and direct inference signature.\n",
    "4. `generate_initial_models` Function:\n",
    "   * Purpose: This function orchestrates the creation and initial\n",
    "     export of all necessary TorchScript models.\n",
    "   * Functionality:\n",
    "       * It sets a fixed random seed (`SEED = 42`) for both PyTorch\n",
    "         and NumPy to ensure reproducibility of initial weights and\n",
    "         data generation.\n",
    "       * It instantiates BurgersPINN and IdentityLoss.\n",
    "       * It then uses torch.jit.script to convert these Python\n",
    "         nn.Module instances into TorchScript (.pt) files:\n",
    "           * burgers_model.pt: The full BurgersPINN model (used for\n",
    "             training in Fortran).\n",
    "           * burgers_loss.pt: The IdentityLoss model (used as a dummy\n",
    "             loss function in Fortran).\n",
    "           * burgers_inference_net.pt: The SimpleInferenceNet (used\n",
    "             for inference in Fortran).\n",
    "\n",
    "In essence, generate_burgers_model.py prepares the necessary deep\n",
    "learning components (the PINN model for training, a pass-through loss\n",
    "function, and a dedicated inference network) by converting them into a\n",
    "format (.pt files) that the TorchFort Fortran library can load and\n",
    "execute efficiently, enabling the Fortran program to perform both PINN\n",
    "training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73394c39-0daf-47df-a2cc-6d610a9fc9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
