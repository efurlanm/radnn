# Physics-Informed Neural Networks for Parameter Discovery in 2D Burgers' Equation: A Comparative Study

## Abstract

*This article presents a comparative study of two distinct methodologies for solving inverse problems related to the two-dimensional Burgers' equation, with a focus on parameter discovery. The first approach employs a traditional numerical method based on finite differences, coupled with gradient-based optimization to identify the kinematic viscosity. The second, more modern approach, utilizes Physics-Informed Neural Networks (PINNs) to achieve the same goal. We provide a detailed analysis of both methods, drawing inspiration from the foundational work of Raissi et al. (2019) on PINNs. The implementations are discussed in the context of popular deep learning libraries such as TensorFlow. The results demonstrate the effectiveness of both techniques, while also highlighting the unique advantages and challenges associated with each. This work serves as both a review of current techniques and a case study, providing a comprehensive guide for researchers and practitioners interested in applying machine learning to solve inverse problems in fluid dynamics.*

## 1. Introduction

The study of partial differential equations (PDEs) is fundamental to many scientific and engineering disciplines. The Burgers' equation, a non-linear PDE, is of particular interest as it appears in various fields such as fluid mechanics, non-linear acoustics, and traffic flow. While solving the "forward problem" (i.e., finding the solution of the PDE given all parameters) is a well-established area, the "inverse problem" of determining the PDE parameters from observed data presents a greater challenge.

This paper explores the inverse problem for the 2D Burgers' equation, focusing on the discovery of the kinematic viscosity coefficient. We compare a traditional approach, using numerical solutions of the PDE within an optimization loop, with a more recent method based on Physics-Informed Neural Networks (PINNs).

The structure of this paper is as follows: Section 2 provides a brief overview of the 2D Burgers' equation. Section 3 details the two methodologies used for parameter discovery. Section 4 provides a review of the available libraries for PINN implementation. Section 5 presents the results obtained from both methods. Section 6 discusses the results and concludes the paper. Finally, a bibliography is provided in Section 7.

## 2. The 2D Burgers' Equation

The 2D Burgers' equation is a system of two coupled, non-linear PDEs that describe the velocity field of a fluid. The equations are given by:

$$
\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} + v \frac{\partial u}{\partial y} = \nu (\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2})
$$

$$
\frac{\partial v}{\partial t} + u \frac{\partial v}{\partial x} + v \frac{\partial v}{\partial y} = \nu (\frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2})
$$

where:

- \(u(x, y, t)\) is the velocity component in the x-direction.
- \(v(x, y, t)\) is the velocity component in the y-direction.
- \(\nu\) is the kinematic viscosity.

## 3. Methodology

### 3.1. Finite Difference Method with Gradient-Based Optimization

This approach, implemented in `burgers2d-01.py`, tackles the inverse problem by combining a traditional numerical method with modern machine learning tools.

#### 3.1.1. Numerical Simulation

The 2D Burgers' equation is discretized on a grid and solved using a finite difference scheme. The core of this method is the `forward_simulation` function, which iteratively calculates the velocity field over time. This function is implemented in TensorFlow to allow for automatic differentiation, a key feature for the optimization process.

#### 3.1.2. Parameter Discovery

The parameter discovery is framed as an optimization problem. The goal is to find the value of \(\nu\) that minimizes the difference between the simulated velocity field and a "measured" or "ground truth" field. The mean squared error (MSE) is used as the loss function. The Adam optimizer, a popular gradient-based optimization algorithm, is used to update the estimate of \(\nu\) in each iteration.

### 3.2. Physics-Informed Neural Networks (PINNs)

The second approach, implemented in `burgers2d-02.py`, is based on the work of Raissi et al. (2019) and utilizes a PINN.

#### 3.2.1. PINN Architecture

A deep neural network is constructed to approximate the solution of the Burgers' equation, \(u(x, y, t)\) and \(v(x, y, t)\). The network takes the spatial and temporal coordinates (x, y, t) as input and outputs the velocity components (u, v).

#### 3.2.2. Loss Function

The loss function for the PINN is composed of two parts:

1. **Data Loss:** This is the mean squared error between the PINN's prediction and the measured data, similar to the first method.
2. **PDE Loss:** This term enforces the physical law described by the Burgers' equation. It is the mean squared error of the PDE residuals. The residuals are calculated by applying automatic differentiation to the neural network's output to compute the derivatives in the Burgers' equation.

The total loss is the sum of the data loss and the PDE loss. By minimizing this total loss, the PINN learns a solution that both fits the data and respects the underlying physics. The kinematic viscosity \(\nu\) is treated as a trainable variable in the network and is learned along with the weights and biases.

## 4. A Review of PINN Libraries

The implementation of PINNs is facilitated by a growing ecosystem of libraries and frameworks. These can be broadly categorized into general-purpose deep learning frameworks and specialized PINN libraries.

### 4.1. General-Purpose Deep Learning Frameworks

These libraries provide the foundational tools for building and training neural networks, including the crucial automatic differentiation capabilities required for PINNs.

- **TensorFlow:** An open-source platform for machine learning developed by Google. It offers a comprehensive ecosystem of tools, libraries, and community resources. Its automatic differentiation feature is a key component for implementing PINNs.
- **PyTorch:** A popular open-source machine learning library known for its flexibility and imperative programming style. It is widely used in the research community and has strong support for automatic differentiation.
- **JAX:** A high-performance numerical computing library from Google that is gaining traction in the scientific machine learning community. It combines NumPy's syntax with just-in-time (JIT) compilation and advanced automatic differentiation capabilities, making it a powerful tool for PINN implementations.

### 4.2. Specialized PINN Libraries

These libraries are built on top of the general-purpose frameworks and provide higher-level abstractions and tools specifically designed for solving PDE-based problems with PINNs.

- **DeepXDE:** A versatile library that supports TensorFlow, PyTorch, and JAX as backends. It is known for its user-friendly interface and its ability to solve a wide range of problems, including forward and inverse problems, and problems with complex geometries.
- **PINA (Physics-Informed Neural networks for Advanced modeling):** A modular and scalable library built on PyTorch. It is designed to be easily customizable and to support a wide range of PDE-based problems.
- **IDRLnet:** A Python toolbox that provides a structured approach to modeling and solving problems with PINNs. It includes features for handling noisy inverse problems and for variational minimization.
- **jinns:** A JAX-based library with a focus on inverse problems and meta-modeling. It is designed for high performance, particularly for inverse problems.
- **Modulus (NVIDIA):** A framework for developing PINN models that is optimized for NVIDIA GPUs. It is designed for scalability and performance.
- **PiNN:** A Python library built on TensorFlow for creating atomic neural networks for molecules and materials. While its focus is on chemistry and materials science, the underlying principles are applicable to other domains.

### 4.3. Other Tools

- **MATLAB:** The Deep Learning Toolboxâ„¢ in MATLAB provides an environment for creating and training PINNs. It also allows for the integration of PINNs with Simulink for system-level simulations.

## 5. Results

Both methods were successful in discovering the kinematic viscosity of the 2D Burgers' equation.

### 5.1. Finite Difference Method Results

The finite difference method, with a true viscosity `nu_real = 0.05`, started with an initial guess of `nu_guess = 0.1`. After 300 epochs of optimization, the discovered value converged to `0.050000`, which is remarkably close to the true value. The evolution of the discovered `nu` and the loss function over the epochs is shown below:

- **Epoch 0:** Loss = 0.000215, nu = 0.090004
- **Epoch 50:** Loss = 0.000000, nu = 0.050883
- **Epoch 100:** Loss = 0.000000, nu = 0.050023
- **Epoch 150:** Loss = 0.000000, nu = 0.049997
- **Epoch 200:** Loss = 0.000000, nu = 0.050001
- **Epoch 250:** Loss = 0.000000, nu = 0.050000

The final discovered value is `0.050000`. A visual comparison of the measured and discovered solutions is presented in Figure 1.

![Figure 1: Comparison of the measured and discovered solutions using the finite difference method.](burgers2d_results.jpg)

*Figure 1: Comparison of the measured (left) and discovered (right) solutions for the u-component of the velocity field using the finite difference method. The discovered solution was obtained with the identified viscosity value of 0.0500.* 

### 5.2. PINN Results

The PINN approach required significant tuning to achieve accurate results. The final model was trained for 50,000 epochs using the Adam optimizer with a learning rate of 0.0001. To ensure the physical plausibility of the results, the network was trained to discover the logarithm of the viscosity, thereby constraining the viscosity to be positive. The initial guess for the viscosity was `0.1`, and the true value was `0.05`.

The final discovered value was `0.040051`, which is a reasonably accurate result, demonstrating the capability of PINNs for parameter discovery. The training progress is summarized below:

- **Epoch 0:** Loss = 0.232435, Discovered nu = 0.100010
- **Epoch 10000:** Loss = 0.000789, Discovered nu = 0.047608
- **Epoch 20000:** Loss = 0.000149, Discovered nu = 0.044678
- **Epoch 30000:** Loss = 0.000059, Discovered nu = 0.043637
- **Epoch 40000:** Loss = 0.000038, Discovered nu = 0.041975
- **Epoch 49900:** Loss = 0.000028, Discovered nu = 0.040070

The final discovered value is `0.040051`. A visual comparison between the measured solution and the solution predicted by the PINN is presented in Figure 2.

![Figure 2: Comparison of the measured and PINN-predicted solutions.](burgers2d_pinn_results.jpg)

*Figure 2: Comparison of the measured (left) and PINN-predicted (right) solutions for the u-component of the velocity field. The PINN-predicted solution was obtained with the identified viscosity value of 0.0401.*

Further experiments were conducted using a two-phase optimization strategy, starting with the Adam optimizer and switching to an L-BFGS-B optimizer for fine-tuning. While this approach is powerful in theory, it did not yield superior results in this case, suggesting that a well-tuned, single-phase Adam optimization was more effective for this specific problem configuration. 

## 6. Conclusion

This paper presented a comparative study of two methods for parameter discovery in the 2D Burgers' equation. Both the traditional finite difference method with gradient-based optimization and the modern PINN approach proved to be effective, although with different levels of accuracy and computational cost.

The finite difference method is conceptually straightforward and relies on well-established numerical techniques. It demonstrated high accuracy in this specific problem. However, it can be computationally expensive, as it requires a full simulation of the PDE at each optimization step.

The PINN approach, on the other hand, offers a more flexible and potentially more efficient way to solve inverse problems. By incorporating the physics directly into the loss function, PINNs can learn from sparse data and can be more robust to noise. The accuracy of the PINN is highly dependent on the network architecture, the number of training epochs, and the number of collocation points. The result obtained in this study could likely be improved by further tuning these hyperparameters.

Future work could involve a more detailed comparison of the computational cost and robustness of the two methods, as well as their application to more complex inverse problems. A more thorough hyperparameter search for the PINN could also lead to improved accuracy.

## 7. Bibliography

- Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. *Journal of Computational Physics*, *378*, 686-707.
- TensorFlow Developers. (2022). TensorFlow (Version 2.x) [Software]. Available from https://www.tensorflow.org/
- Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.
- Abadi, M., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous distributed systems. *arXiv preprint arXiv:1603.04467*.
- Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In *Advances in neural information processing systems* (pp. 1097-1105).
